import pytest
import os
import json
from dotenv import load_dotenv

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì „ .env ë¡œë“œ
load_dotenv(dotenv_path="backend/.env")

# Agent ëª¨ë“ˆ Import
from app.ai.agents import main_agent

@pytest.mark.asyncio
async def test_integration_flow():
    """
    [Integration Test] MCP Server Main Flow
    1. start_interview: ì£¼ì œ ì¶”ì²œ
    2. generate_questions: ë¬¸ì œ ìƒì„±
    3. evaluate_answer: ë‹µë³€ í‰ê°€ (Pass & Fail ì‹œë‚˜ë¦¬ì˜¤)
    4. summarize_result: ê²°ê³¼ ë¦¬í¬íŠ¸
    """
    print("\nğŸš€ [Integration Test] Starting MCP Agents Integration Flow...")

    # --- Step 1: Start Interview (Topic Recommendation) ---
    print("\nğŸ“ [Step 1] Testing start_interview...")
    user_greet = "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ë°±ì—”ë“œ ê°œë°œìì¸ë° AI ì§€ì‹ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ì‹¶ì–´ìš”."
    
    # Tool ê°ì²´ëŠ” ì§ì ‘ í˜¸ì¶œì´ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ .ainvoke ì‚¬ìš©
    recommendation = await main_agent.start_interview.ainvoke({"user_input": user_greet})
    
    print(f"User: {user_greet}")
    print(f"AI Recommendation: {recommendation}")
    
    assert recommendation is not None
    assert len(recommendation) > 20

    # --- Step 2: Generate Questions ---
    print("\nğŸ“ [Step 2] Testing generate_questions...")
    topic = "Python"
    level = "Basic"
    count = 2
    
    questions = await main_agent.generate_questions.ainvoke({
        "topic": topic, 
        "level": level, 
        "count": count
    })
    
    print(f"Generated {len(questions)} questions on {topic} ({level})")
    print(json.dumps(questions[0], indent=2, ensure_ascii=False))
    
    assert isinstance(questions, list)
    assert len(questions) == count
    assert "question_text" in questions[0]
    assert "model_answer" in questions[0]
    
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ ë¬¸ì œ ê°€ì ¸ì˜¤ê¸°
    target_question = questions[0]
    q_text = target_question["question_text"]

    # --- Step 3: Evaluate Answer (Fail / Deep Dive Scenario) ---
    print("\nğŸ“ [Step 3] Testing evaluate_answer (Weak Answer)...")
    user_weak_answer = "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤. ê·¸ëƒ¥ ë¦¬ìŠ¤íŠ¸ ê°™ì€ ê±° ì•„ë‹Œê°€ìš”?"
    
    eval_result_weak = await main_agent.evaluate_answer.ainvoke({
        "question": q_text,
        "user_answer": user_weak_answer,
        "level": level
    })
    
    print(f"Weak Answer Result: {json.dumps(eval_result_weak, indent=2, ensure_ascii=False)}")
    
    assert eval_result_weak["is_pass"] is False
    assert eval_result_weak["score"] < 70
    assert eval_result_weak["next_action"] == "DEEP_DIVE"
    # Interviewerê°€ ìƒì„±í•œ ì¹œì ˆí•œ í”¼ë“œë°± ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
    assert len(eval_result_weak["feedback"]) > 10

    # --- Step 5: Summarize Result ---
    print("\nğŸ“ [Step 5] Testing summarize_result...")
    
    history_log = [
        f"Q: {q_text}",
        f"A: {user_weak_answer}",
        f"Eval: {eval_result_weak['score']}ì , {eval_result_weak['feedback']}"
    ]
    
    report = await main_agent.summarize_result.ainvoke({"conversation_history": history_log})
    
    print("Final Report:")
    print(report)
    
    assert report is not None
    assert len(report) > 50
    # Markdown í˜•ì‹ ì²´í¬ (Rough)
    # assert "#" in report or "-" in report

    print("\nâœ… [Success] All agents are communicating correctly!")

if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])

# source .venv/bin/activate
# pytest tests/test_integration_flow.py -v -s