import streamlit as st
import requests
import json
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# Load .env (Try looking in parent dirs if needed, but assuming backend/.env based on execution path)
load_dotenv()
# If .env inside backend/.env
load_dotenv("backend/.env")

# -------------------------------------------------------------------------
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool

# -------------------------------------------------------------------------
# Configuration
# -------------------------------------------------------------------------
MCP_SERVER_URL = "http://localhost:8100"
st.set_page_config(page_title="AI TechTree MCP", page_icon="ğŸŒ³", layout="wide")

st.title("ğŸ¤– AI TechTree MCP")
st.caption("ë…ë¦½ëœ MCP ì„œë²„(Port 8100)ì™€ í†µì‹ í•˜ë©° ìŠ¤ìŠ¤ë¡œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ë‹µë³€í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.")

# -------------------------------------------------------------------------
# Remote Tool Definitions (Client Side Proxies)
# -------------------------------------------------------------------------
# ì—ì´ì „íŠ¸ê°€ "ì´ ë„êµ¬ë¥¼ ì¨ì•¼ê² ë‹¤"ê³  íŒë‹¨í•˜ë©´, ì‹¤ì œ ì‹¤í–‰ì€ MCP ì„œë²„ë¡œ ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.

@tool
def client_get_ai_track(interests: list[str], experience_level: str) -> dict:
    """
    ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ì™€ ê²½ë ¥ ìˆ˜ì¤€ì„ ê¸°ë°˜ìœ¼ë¡œ ì í•©í•œ AI ì»¤ë¦¬í˜ëŸ¼ íŠ¸ë™ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    Args:
        interests: ê´€ì‹¬ ë¶„ì•¼ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë°ì´í„° ë¶„ì„", "ì›¹ ê°œë°œ"])
        experience_level: ê²½ë ¥ ìˆ˜ì¤€ ("beginner", "intermediate", "expert")
    """
    payload = {"input": {"interests": interests, "experience_level": experience_level}}
    try:
        # MCP Server Call
        response = requests.post(f"{MCP_SERVER_URL}/get_ai_track/invoke", json=payload, timeout=10)
        response.raise_for_status()
        return response.json().get("output", {})
    except Exception as e:
        return {"error": str(e)}

@tool
def client_get_ai_path(track_name: str) -> dict:
    """
    íŠ¹ì • AI íŠ¸ë™ì˜ ìƒì„¸ ì»¤ë¦¬í˜ëŸ¼(ë¡œë“œë§µ/ê³µë¶€ ìˆœì„œ)ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    Args:
        track_name: íŠ¸ë™ ì´ë¦„ (ì˜ˆ: "Track 1: AI Engineer")
    """
    payload = {"input": {"track_name": track_name}}
    try:
        response = requests.post(f"{MCP_SERVER_URL}/get_ai_path/invoke", json=payload, timeout=10)
        response.raise_for_status()
        return response.json().get("output", {})
    except Exception as e:
        return {"error": str(e)}

@tool
def client_get_ai_trend(keywords: list[str], category: str = "tech_news") -> list:
    """
    ìµœì‹  AI ê¸°ìˆ  íŠ¸ë Œë“œë‚˜ ë‰´ìŠ¤ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    Args:
        keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        category: ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬ ("tech_news", "engineering", "research", "k_blog")
    """
    payload = {"input": {"keywords": keywords, "category": category}}
    try:
        response = requests.post(f"{MCP_SERVER_URL}/get_ai_trend/invoke", json=payload, timeout=10)
        response.raise_for_status()
        return response.json().get("output", [])
    except Exception as e:
        return [{"error": str(e)}]

# Available Tools for the Agent
tools = [client_get_ai_track, client_get_ai_path, client_get_ai_trend]

# -------------------------------------------------------------------------
# Agent Setup
# -------------------------------------------------------------------------
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! AI ì»¤ë¦¬í˜ëŸ¼ ì¶”ì²œ, ë¡œë“œë§µ ì¡°íšŒ, ìµœì‹  íŠ¸ë Œë“œ ê²€ìƒ‰ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

# Initialize LLM with Tools
llm = ChatOpenAI(model="gpt-4o", temperature=0.2)
llm_with_tools = llm.bind_tools(tools)

# -------------------------------------------------------------------------
# Chat Interface Logic
# -------------------------------------------------------------------------

# Display Chat History
for msg in st.session_state["messages"]:
    if isinstance(msg, HumanMessage):
        st.chat_message("user").write(msg.content)
    elif isinstance(msg, AIMessage):
        st.chat_message("assistant").write(msg.content)
    elif isinstance(msg, ToolMessage):
        with st.expander(f"ğŸ› ï¸ Tool Output: {msg.name}"):
            st.code(msg.content, language="json")

# Handle User Input
if prompt := st.chat_input("ì˜ˆ: ë°ì´í„° ë¶„ì„ì— ê´€ì‹¬ ìˆëŠ” ì´ˆë³´ìì¸ë° ê³µë¶€ ìˆœì„œ ì•Œë ¤ì¤˜"):
    # 1. User Message
    st.chat_message("user").write(prompt)
    st.session_state["messages"].append(HumanMessage(content=prompt))

    # 2. Agent Reasoning loop
    with st.chat_message("assistant"):
        with st.spinner("AIê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
            # First LLM Call (Decide Tool)
            response = llm_with_tools.invoke(st.session_state["messages"])
            st.session_state["messages"].append(response)
            
            # Check if tool usage is requested
            if response.tool_calls:
                # Execute Tools
                for tool_call in response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]
                    tool_id = tool_call["id"]
                    
                    st.toast(f"ğŸ› ï¸ {tool_name} ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...", icon="ğŸ”§")
                    
                    # Find matching client tool
                    selected_tool = next((t for t in tools if t.name == tool_name), None)
                    if selected_tool:
                        tool_result = selected_tool.invoke(tool_args)
                        
                        # Show Tool Output (Intermediate)
                        with st.expander(f"âš¡ Tool Execution: {tool_name}"):
                            st.json(tool_result)
                            
                        # Append Tool Message to History
                        tool_msg = ToolMessage(
                            tool_call_id=tool_id,
                            name=tool_name,
                            content=str(tool_result)  # Must be string
                        )
                        st.session_state["messages"].append(tool_msg)
                
                # Final LLM Call (Generate Answer based on Tool Output)
                # Use streaming for better UX
                stream_handler = st.chat_message("assistant").empty()
                final_content = ""
                
                for chunk in llm_with_tools.stream(st.session_state["messages"]):
                    if isinstance(chunk, AIMessage) and chunk.content:
                         final_content += chunk.content
                         stream_handler.markdown(final_content + "â–Œ")
                
                stream_handler.markdown(final_content)
                st.session_state["messages"].append(AIMessage(content=final_content))
                
            else:
                # No tool needed, just chat
                st.chat_message("assistant").write(response.content)
