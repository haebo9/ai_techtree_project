from typing import Dict, List, Any, Optional
import asyncio

from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage

from app.ai.agents import qamaker_agent, interviewer_agent, evaluator_agent
# from app.core.config import settings

# -------------------------------------------------------------------------
# MCP Tools Definition
# Main Agent acts as a provider/registry of these tools.
# -------------------------------------------------------------------------

@tool("generate_questions", description="Generate interview questions based on topic and difficulty level.")
async def generate_questions(topic: str, level: str, count: int = 1) -> List[Dict[str, Any]]:
    """
    Generate interview questions based on a topic and difficulty level.
    Use this tool when the user wants to start a new quiz or interview session.
    """
    print(f"ğŸ•µï¸â€â™‚ï¸ [Tool:generate_questions] Generating {count} questions for {topic} ({level})...")
    
    # QAMakerì—ê²Œ í•œ ë²ˆì— ìš”ì²­ (ì¤‘ë³µ ë°©ì§€ ë° ìµœì í™”)
    try:
        results = await qamaker_agent.generate_questions(
            skill=topic, 
            topic=topic, 
            level=level, 
            count=count
        )
        
        if not results:
            return [{"error": "Failed to generate questions. Please try again."}]
            
        return results
        
    except Exception as e:
        print(f"Error in generate_questions tool: {e}")
        return [{"error": f"An error occurred: {str(e)}"}]


@tool("evaluate_answer", description="Evaluate user Answer and decide Next Action (PASS/DEEP_DIVE).")
async def evaluate_answer(question: str, user_answer: str, level: str) -> Dict[str, Any]:
    """
    Evaluate the user's answer and decide the next course of action.
    Use this tool immediately after the user provides an answer to an interview question.
    """
    print(f"ğŸ¤” [Tool:evaluate_answer] Evaluating answer for {level}...")

    # 1. Evaluator: ì ìˆ˜ ë° íŒ©íŠ¸ ì²´í¬ (Judge)
    eval_result = await evaluator_agent.evaluate_answer(
        question=question,
        user_answer=user_answer,
        model_answer="N/A", 
        evaluation_criteria=[f"Level: {level}"]
    )
    
    score = eval_result.get("score", 0)
    is_pass = eval_result.get("is_passed", False)
    eval_feedback = eval_result.get("feedback", "")
    
    # 2. Interviewer: í”¼ë“œë°± ë©˜íŠ¸ ë° ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± (Persona/Writer)
    final_message = await interviewer_agent.generate_feedback_message(
        question=question,
        user_answer=user_answer,
        score=score,
        is_pass=is_pass,
        feedback=eval_feedback
    )

    # 3. Next Action ê²°ì •
    # ì ìˆ˜ê°€ ë‚®ê±°ë‚˜(Fail), ì ìˆ˜ëŠ” ë†’ì§€ë§Œ ê²€ì¦ì´ ë” í•„ìš”í•˜ë‹¤ëŠ” ë‰˜ì•™ìŠ¤(ê¼¬ë¦¬ì§ˆë¬¸)ê°€ ìˆë‹¤ë©´ DEEP_DIVE
    next_action = "PASS" if is_pass else "DEEP_DIVE"
    
    return {
        "score": score,
        "feedback": final_message, # ë‹¨ìˆœ Factê°€ ì•„ë‹Œ Interviewerê°€ ê°€ê³µí•œ ì¹œì ˆí•œ ë©˜íŠ¸
        "is_pass": is_pass,
        "next_action": next_action
    }

@tool("start_interview", description="Initiate the interview session and recommend topics.")
async def start_interview(user_input: str) -> str:
    """
    Start the interview session.
    Use this tool when the user greets or asks for an interview without a specific ongoing topic.
    It will analyze the user's intent and recommend suitable interview topics from the curriculum.
    """
    print(f"ğŸ‘‹ [Tool:start_interview] User Input: {user_input}")
    
    # Interviewerì—ê²Œ ì»¤ë¦¬í˜ëŸ¼ ê¸°ë°˜ ì¶”ì²œ ë©˜íŠ¸ ìƒì„±ì„ ìš”ì²­
    response = await interviewer_agent.recommend_topic_response(user_input)
    
    return response


@tool("summarize_result", description="Analyze conversation logic and generate a final report.")
async def summarize_result(conversation_history: List[str]) -> str:
    """
    Analyze the full conversation history and generate a comprehensive final report.
    Use this tool when the interview session is finished.
    """
    full_log = "\n".join(conversation_history)
    
    # 1. Evaluator: ë¡œì§ ë¶„ì„
    report_prompt = f"""
    [ë¡œê·¸]
    {full_log}
    
    ìœ„ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ê°•ì , ì•½ì , ì¢…í•© ì ìˆ˜ë¥¼ ë„ì¶œí•˜ì„¸ìš”.
    """
    raw_analysis = await evaluator_agent.llm.ainvoke([HumanMessage(content=report_prompt)])
    
    # 2. Interviewer: ìµœì¢… ë¦¬í¬íŠ¸ í¬ë§·íŒ…
    final_report = await interviewer_agent.format_final_report(raw_analysis.content)
    
    return final_report

# List of tools exported for easy registration
TOOLS = [start_interview, generate_questions, evaluate_answer, summarize_result]

